import os
import torch
from datasets import Dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    BitsAndBytesConfig
)
from peft import LoraConfig, get_peft_model
from bs4 import BeautifulSoup
import re
import signal
import sys

# 1. Конфигурация
EPUB_DIR = os.path.expanduser("~/exist_train")  # Папка с книгами
CHUNK_SIZE = 512
MAX_SAMPLES = 16384
MODEL_NAME = "ai-forever/rugpt3small_based_on_gpt2"  

print(f"EPUB-файлы должны быть размещены в: {EPUB_DIR}")

# 2. Функция для конвертации EPUB в текст
def epub_to_text(path):
    try:
        import zipfile
        import xml.etree.ElementTree as ET
        from bs4 import BeautifulSoup
        import re
        import os
        
        ns = {
            'n': 'urn:oasis:names:tc:opendocument:xmlns:container',
            'opf': 'http://www.idpf.org/2007/opf'
        }
        
        full_text = []
        
        with zipfile.ZipFile(path) as z:
            if 'mimetype' not in z.namelist() or z.read('mimetype').decode() != 'application/epub+zip':
                print(f"  {os.path.basename(path)} не является валидным EPUB-файлом")
                return ""
                
            container = ET.fromstring(z.read('META-INF/container.xml'))
            rootfile = container.find('.//n:rootfile', ns)
            if rootfile is None:
                print(f"  Не найден rootfile в {os.path.basename(path)}")
                return ""
                
            opf_path = rootfile.attrib['full-path']
            opf_dir = os.path.dirname(opf_path)
            
            opf = ET.fromstring(z.read(opf_path))
            manifest = opf.find('opf:manifest', ns)
            if manifest is None:
                print(f"  Не найден manifest в {os.path.basename(path)}")
                return ""
                
            html_files = []
            for item in manifest.findall('opf:item', ns):
                if item.attrib['media-type'] in ['application/xhtml+xml', 'text/html']:
                    html_files.append(os.path.join(opf_dir, item.attrib['href']))
            
            for html_file in html_files:
                try:
                    content = z.read(html_file)
                    soup = BeautifulSoup(content, 'html.parser')
                    text = soup.get_text()
                    
                    text = re.sub(r'\s+', ' ', text)
                    text = re.sub(r'[^\w\s.,!?;:—–-]', '', text)
                    text = text.strip()
                    
                    if text:
                        full_text.append(text)
                except Exception as e:
                    print(f"  Ошибка обработки {html_file}: {str(e)}")
        
        return " ".join(full_text)
    except Exception as e:
        print(f"Ошибка чтения {os.path.basename(path)}: {str(e)}")
        return ""

# 3. Загрузка и обработка EPUB-файлов
def load_epub_dataset():
    chunks = []
    
    epub_files = [f for f in os.listdir(EPUB_DIR) if f.lower().endswith('.epub')]
    if not epub_files:
        raise FileNotFoundError(f"В папке {EPUB_DIR} не найдено EPUB-файлов!")
    
    print(f"Найдено {len(epub_files)} EPUB-файлов для обработки")
    
    for filename in epub_files:
        filepath = os.path.join(EPUB_DIR, filename)
        print(f"Обработка: {filename}")
        
        text = epub_to_text(filepath)
        if not text:
            print(f"  Пропущен (пустой текст): {filename}")
            continue
            
        for i in range(0, len(text), CHUNK_SIZE):
            chunk = text[i:i+CHUNK_SIZE]
            if len(chunk) > 100:
                chunks.append({"text": chunk})
                
            if len(chunks) >= MAX_SAMPLES:
                break
                
        print(f"  Добавлено фрагментов: {min(len(text)//CHUNK_SIZE, MAX_SAMPLES - len(chunks))}")
        
        if len(chunks) >= MAX_SAMPLES:
            print(f"Достигнут максимум в {MAX_SAMPLES} фрагментов")
            break
        if sys.getsizeof(chunks) > 1_000_000_000:
            print(f"Превышен лимит памяти. Используется первых {len(chunks)//2} фрагментов")

    print(f"Итого создано {len(chunks)} текстовых фрагментов")
    return Dataset.from_list(chunks)

# 4. Загрузка токенизатора (изменили модель)
print("\nЗагрузка токенизатора...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
tokenizer.pad_token = tokenizer.eos_token 

# 5. Подготовка датасета (без изменений)
print("\nПодготовка датасета...")
dataset = load_epub_dataset()

def tokenize_function(examples):
    tokenized = tokenizer(
        examples["text"],
        truncation=True,
        max_length=256,
        padding="max_length",
        return_tensors="pt"
    )
    tokenized["labels"] = tokenized["input_ids"].clone() 
    return tokenized

tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=["text"])


# 6. Конфигурация Quantization
quant_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=False
)

# 7. Загрузка модели
print(f"\nЗагрузка модели {MODEL_NAME}...")
model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, device_map="auto")

# 8. Настройка LoRA для GPT-3
peft_config = LoraConfig(
    r=8,
    lora_alpha=32,
    target_modules=[
        "c_attn", 
        "c_proj",  
        "c_fc",    
        "mlp.c_proj" 
    ],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

model = get_peft_model(model, peft_config)
model.print_trainable_parameters()

# 9. Параметры обучения
training_args = TrainingArguments(
    output_dir="./rugpt3small-trained",
    per_device_train_batch_size=2,
    gradient_accumulation_steps=2,
    num_train_epochs=3,
    learning_rate=2e-5,
    optim="adamw_torch",
    logging_steps=10,
    save_strategy="steps",
    save_steps=200,
    report_to="none",
    remove_unused_columns=False,
    fp16=True if torch.cuda.is_available() else False
)

# 10. Запуск обучения
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
)

print("\nСтарт обучения модели...")
trainer.train()

# 11. Сохранение
model.save_pretrained("./rugpt3small-trained")
tokenizer.save_pretrained("./rugpt3small-trained")
print("Модель сохранена в './rugpt3small-trained'")

# 12. Тестирование
input_text = "В чём смысл жизни?"
inputs = tokenizer(input_text, return_tensors="pt").to("cuda" if torch.cuda.is_available() else "cpu")
outputs = model.generate(
    **inputs,
    max_new_tokens=50,
    temperature=0.7,
    do_sample=True,
    pad_token_id=tokenizer.eos_token_id
)
print("\nРезультат генерации:")
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
