import os
import torch
import json
import zipfile
import gc
from typing import Dict, Any
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
from peft import LoraConfig, get_peft_model
from datasets import Dataset
import re
import numpy as np
from rouge_score import rouge_scorer
from transformers import EarlyStoppingCallback

# Очистка памяти
def clear_memory():
    torch.cuda.empty_cache()
    gc.collect()

# Конфигурация
MODEL_NAME = "Defetya/qwen-1.8B-saiga"
INPUT_DIR = "/kaggle/input/exist-train/"
OUTPUT_DIR = "/kaggle/working/"
CHUNK_SIZE = 256
MAX_SAMPLES = 16384
BATCH_SIZE = 1
GRAD_ACCUM_STEPS = 8
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

def epub_to_text(path):
    try:
        import xml.etree.ElementTree as ET
        from bs4 import BeautifulSoup

        ns = {
            'n': 'urn:oasis:names:tc:opendocument:xmlns:container',
            'opf': 'http://www.idpf.org/2007/opf',
            'dc': 'http://purl.org/dc/elements/1.1/'
        }

        full_text = []

        with zipfile.ZipFile(path) as z:
            if 'mimetype' not in z.namelist() or z.read('mimetype').decode() != 'application/epub+zip':
                return ""

            container = ET.fromstring(z.read('META-INF/container.xml'))
            rootfile = container.find('.//n:rootfile', ns)
            if rootfile is None:
                return ""

            opf_path = rootfile.attrib['full-path']
            opf_dir = os.path.dirname(opf_path)
            opf = ET.fromstring(z.read(opf_path))
            spine = opf.find('opf:spine', ns)
            if spine is None:
                return ""

            itemrefs = spine.findall('opf:itemref', ns)
            if not itemrefs:
                return ""

            start_index = 2 if len(itemrefs) > 2 else 0
            manifest = opf.find('opf:manifest', ns)
            if manifest is None:
                return ""

            id_to_file = {}
            for item in manifest.findall('opf:item', ns):
                id_to_file[item.attrib['id']] = os.path.join(opf_dir, item.attrib['href'])

            for itemref in itemrefs[start_index:]:
                item_id = itemref.attrib['idref']
                if item_id not in id_to_file:
                    continue

                html_file = id_to_file[item_id]
                try:
                    content = z.read(html_file)
                    soup = BeautifulSoup(content, 'html.parser')

                    for element in soup(['header', 'nav', 'toc', 'table-of-contents', 'script', 'style']):
                        element.decompose()

                    text = soup.get_text()
                    text = re.sub(r'\s+', ' ', text)
                    text = re.sub(r'[^\w\s.,!?;:—–-]', '', text)
                    text = text.strip()

                    if text and len(text) > 50:
                        full_text.append(text)
                except Exception:
                    continue

        return " ".join(full_text)
    except Exception:
        return ""

def load_dataset():
    chunks = []
    supported_files = []

    for root, _, files in os.walk(INPUT_DIR):
        for f in files:
            if f.lower().endswith('.epub'):
                supported_files.append(os.path.join(root, f))

    if not supported_files:
        available_files = "\n".join(os.listdir(INPUT_DIR))
        raise FileNotFoundError(
            f"В папке {INPUT_DIR} не найдено EPUB файлов!\n"
            f"Доступные файлы:\n{available_files}"
        )

    for filepath in supported_files:
        text = epub_to_text(filepath)
        if not text:
            continue

        for i in range(0, len(text), CHUNK_SIZE):
            chunk = text[i:i+CHUNK_SIZE]
            if len(chunk) > 50:
                chunks.append({"text": chunk})

            if len(chunks) >= MAX_SAMPLES:
                break

        if len(chunks) >= MAX_SAMPLES:
            break

    return Dataset.from_list(chunks[:MAX_SAMPLES])

print("\n=== Загрузка данных ===")
dataset = load_dataset()
clear_memory()

print("\n=== Токенизация данных ===")
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
tokenizer.pad_token = tokenizer.eos_token

def tokenize_function(examples):
    # Токенизируем текст без добавления labels
    return tokenizer(
        examples["text"],
        padding="max_length",
        truncation=True,
        max_length=CHUNK_SIZE,
    )

tokenized_dataset = dataset.map(
    tokenize_function,
    batched=True,
    batch_size=4,
    remove_columns=["text"],
)
clear_memory()

print("\n=== Инициализация модели ===")
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=torch.float16,
)

model = model.to(DEVICE)
model.gradient_checkpointing_enable()
model.config.use_cache = False

peft_config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

model = get_peft_model(model, peft_config)
model.print_trainable_parameters()
clear_memory()

print("\n=== Настройка обучения ===")

class RougeMetrics:
    def __init__(self, tokenizer):
        self.tokenizer = tokenizer
        self.scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
    
    def compute(self, preds, refs):
        scores = []
        for pred, ref in zip(preds, refs):
            scores.append(self.scorer.score(ref, pred))
        return {
            'rouge1': np.mean([s['rouge1'].fmeasure for s in scores]),
            'rouge2': np.mean([s['rouge2'].fmeasure for s in scores]),
            'rougeL': np.mean([s['rougeL'].fmeasure for s in scores])
        }

class ExistentialTrainer(Trainer):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.tokenizer = tokenizer
        self.rouge = RougeMetrics(tokenizer)
        self.metrics_file = os.path.join(self.args.output_dir, "training_metrics.json")
        
    def compute_metrics(self, eval_preds):
        preds, labels = eval_preds
        
        # Декодируем предсказания и метки
        decoded_preds = self.tokenizer.batch_decode(preds, skip_special_tokens=True)
        decoded_labels = self.tokenizer.batch_decode(labels, skip_special_tokens=True)
        
        # Фильтрация пустых строк
        decoded_preds = [p.strip() for p in decoded_preds if p.strip()]
        decoded_labels = [l.strip() for l in decoded_labels if l.strip()]
        
        # Вычисляем ROUGE
        rouge_scores = {
            'rouge1': [],
            'rouge2': [],
            'rougeL': []
        }
        
        for pred, label in zip(decoded_preds, decoded_labels):
            scores = self.rouge_scorer.score(label, pred)
            rouge_scores['rouge1'].append(scores['rouge1'].fmeasure)
            rouge_scores['rouge2'].append(scores['rouge2'].fmeasure)
            rouge_scores['rougeL'].append(scores['rougeL'].fmeasure)
        
        # Вычисляем BLEU
        refs = [[ref.split()] for ref in decoded_labels]
        preds = [pred.split() for pred in decoded_preds]
        bleu_score = corpus_bleu(refs, preds, smoothing_function=self.smoothing)
        
        return {
            'rouge1': np.mean(rouge_scores['rouge1']),
            'rouge2': np.mean(rouge_scores['rouge2']),
            'rougeL': np.mean(rouge_scores['rougeL']),
            'bleu': bleu_score,
            'samples': list(zip(decoded_labels[:3], decoded_preds[:3]))
        }

data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False,
)

train_size = int(0.9 * len(tokenized_dataset))
eval_size = len(tokenized_dataset) - train_size
train_dataset = tokenized_dataset.select(range(train_size))
eval_dataset = tokenized_dataset.select(range(train_size, train_size + eval_size))

training_args = TrainingArguments(
    output_dir=os.path.join(OUTPUT_DIR, "results"),
    per_device_train_batch_size=BATCH_SIZE,
    gradient_accumulation_steps=GRAD_ACCUM_STEPS,
    per_device_eval_batch_size=1,
    optim="adamw_torch",
    fp16=True,
    learning_rate=5e-5,
    max_grad_norm=0.3,
    eval_strategy="steps",
    eval_steps=250,
    logging_steps=10,
    save_strategy="steps",
    save_steps=50,
    weight_decay=0.01,
    gradient_checkpointing=True,
    save_total_limit=2,
    max_steps=5000,
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    greater_is_better=False,
    report_to="none",
)

trainer = ExistentialTrainer(
    model=model,
    processing_class=tokenizer,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    data_collator=data_collator,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]
)

try:
    print("Начало обучения...")
    trainer.train()
    
    # Сохранение модели
    trainer.save_model(os.path.join(OUTPUT_DIR, "final_model"))
    tokenizer.save_pretrained(os.path.join(OUTPUT_DIR, "final_model"))
    
    # Оценка
    eval_results = trainer.evaluate()
    print("\nРезультаты оценки:")
    print(f"Loss: {eval_results['eval_loss']:.4f}")
    print(f"ROUGE-1: {eval_results.get('rouge1', 0):.4f}")
    print(f"ROUGE-L: {eval_results.get('rougeL', 0):.4f}")
    
    # Пример генерации
    print("\nПример генерации:")
    input_text = "Что такое свобода?"
    inputs = tokenizer(input_text, return_tensors="pt").to(DEVICE)
    outputs = model.generate(**inputs, max_new_tokens=50)
    print(tokenizer.decode(outputs[0], skip_special_tokens=True))
    
except Exception as e:
    print(f"\nОшибка при обучении: {str(e)}")
finally:
    clear_memory()
    print("\nОбучение завершено")
